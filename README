Bolt Online Learning Toolbox
============================

Bolt features discriminative learning of linear predictors (e.g. SVM or
logistic regression) using stochastic gradient descent. Bolt is
aimed at large, high-dimensional and sparse machine-learning problems.
In particular, problems encountered in information retrieval and
natural language processing. 

Bolt considers linear prediction problems where one wants to learn a
linear predictor f(x) which minimizes a given error function E(w),  

   f(x) = w*x + b

Where, x is the example to be predicted, w is a vector of parameters
(aka the weight vector) which specifiy the predictor, b is the bias
term and w*x represents the inner (dot) product of w and x. The error
function takes the following form, 

   E(w) = 'loss + penalty'

Where, 'penalty' is a term which penalizes model complexity (usually
the l2-norm of the weight vector w) and 'loss' is a
convex loss function such as hinge loss or squared error. Popular
linear models such as ridge regression, logistic regression or
(linear) support vector machines can be expressed in the above
framework.

Bolt features: 

   * Fast learning based on stochastic gradient descent with adaptive
     learning rate and some engineering tricks. 

   * Different loss functions for classification and regression. 

   * Different penalties (e.g. L2, L1, elastic-net). 

   * Simple, yet powerful commandline interface similar to SVM^light.

   * Python bindings, feature vectors encoded as numpy arrays. 

The toolkit is written in Python [1], the critical sections are
C-extensions written in Cython [2]. It makes heavy use of numpy [3], a
numeric computing library for python. 

Requirements
------------

To install Bolt you need:

   * Python 2.5 or 2.6
   * C-compiler (tested with gcc 4.3.3)
   * Numpy (tested with 1.2.1)

If you want to modify bolt.pyx you also need cython (>=0.11.2).

Installation
------------

To build bolt simply run,

   python setup.py build

To install bolt on your system, use

   python setup.py install

TODOS
-----

* Proper learning rate for regression.

* Progress handler to monitor learning progress (e.g. learning curves). 



References
----------

[1] http://www.python.org

[2] http://www.cython.org

[3] http://numpy.scipy.org/


[Shwartz, S. S., Singer, Y., and Srebro, N., 2007] Pegasos: Primal
estimated sub-gradient solver for svm. In ICML '07: Proceedings of the
24th international conference on Machine learning, pages 807-814, New
York, NY, USA. ACM. 

[Zhang, T., 2004] Solving large scale linear prediction problems using
stochastic gradient descent algorithms. In ICML '04: Proceedings of
the twenty-first international conference on Machine learning, pages
116+, New York, NY, USA. ACM. 

[Tsuruoka, Y., Tsujii, J., and Ananiadou, S., 2009] Stochastic gradient
descent training for l1-regularized log-linear models with cumulative
penalty. In Proceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages 477-485, Suntec,
Singapore. Association for Computational Linguistics.  
